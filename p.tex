\documentclass[preprint]{elsarticle}
\biboptions{round, numbers}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{textcomp}
\usepackage{graphicx}
%\usepackage{color}
%\usepackage{setspace}
\usepackage{url}
\usepackage[english]{babel}
\usepackage{todonotes}

\begin{document}

\begin{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Controlling URL accesses in the enterprise by means of user-focused categorical classifiers}
% No se deben usar acrónimos en el título - JJ
% Antonio - He puesto un título más serio y acorde con lo que se pretende hacer en el trabajo. El acrónimo de URL no lo vamos a desplegar, digo yo. Si queréis poned Web, pero vamos, que yo no lo veo mal así.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{P. de las Cuevas, A.M. Mora, J.J. Merelo}
\ead{\{paloma, amorag, jmerelo\}@geneura.ugr.es}
\address{Departamento de Arquitectura y Tecnología de Computadores.\\ ETSIIT - CITIC. University of Granada, Spain}
%\author{A. M. Mora}
%\ead{amorag@geneura.ugr.es}
%\address{Departamento de Arquitectura y Tecnología de Computadores. Escuela Técnica Superior de Ingenierías Informática y de Telecomunicación. CITIC. University of Granada, Spain}

%\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 

Many companies, concerned about the safety of the connections their employees establish to different web sites, implement different security techniques. 
One of the most extended involves use of blacklists and whitelists which, respectively, forbid or allow the access to the URLs included on them.
Their main problem is the difficulty for maintaining them up to date, along with the low flexibility they offer.
In this paper it is proposed a methodology for classifying unknown URLs as allowed or denied, based in previous accesses from the users to other sites.
This has been done using categorical classifiers, also taking into account the user influence in the accesses. Thus, a subset of the considered features have been selected based in some criteria, such as their influence on the classifiers' obtained rules and on their dependence on the user's actions.
This has been tested using real data gathered in an actual Spanish company, both at the access request and at the session level.
The obtained results show that \textbf{COMENTAR LOS RESULTADOS OBTENIDOS}. %revisar que el resumen sea correcto (objetivos) y completar comentando en una frase los resultados obtenidos.  [Pedro]
% Antonio - he intentado describir mejor lo que se hace (o se quiere hacer) en el trabajo.

\end{abstract}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   KEYWORDS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{keyword}
Black List \sep White Lists \sep Data Mining \sep Corporate Security Policies \sep URL request\sep User-based system \sep Classification
\end{keyword}

% Antonio - hay que elegir una terminología para las 'blacklists' y las 'whitelists' y usar siempre la misma. A veces están separadas las palabras, otras juntas, unas veces en mayúscula y otras en minúscula...

\end{frontmatter}


%-------------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------------------

\section{Introduction}
\label{sec:intro}

The concept of Security inside an enterprise can be addressed from the
point of view of the Internet connections that are daily being
made. The employees who make these might have working purposes or
not. The enterprise, which is aware of this situation, wants to reduce
the risks of security attacks introduced by some non trustworthy
websites. Then, on a second, but still important level, companies want
to keep their employees productive and focused. % No acabo de entender
                                % el uso de mayúsculas - JJ
Above all the security tools that enterprises normally use, this paper is focused on the widely extended blacklists and whitelists. %Sigo sin
                                %entenderlo. Si es una definición,
                                %cursivas - JJ
% Antonio - intenta usar lo menos posible la primera persona, salvo cuando vayas a dar una opinión o comentar alguna decisión que se haya tomado para el desarrollo del artículo.
They are commonly used in corporate security as it is easy to maintain them, 
                                % easy to maintain? - JJ
                                % (Paloma) sí, es correcto, pero lo cambio si no se entiende bien
                                % Si se dice que se usan mucho, debería ponerse alguna referencia  [Pedro]
and each one has its advantages and disadvantages. 
% Antonio - no dejes frases 'a medias' como esta, complétala diciendo en este caso las ventajas y desventajas, si no, no lo menciones.
The use of both highly increases the security in a company.
Indeed it is true that both blacklists and whitelists are easy to update or to maintain, since every day new dangerous webs are reported. 
% Antonio - intenta usar términos precisos. 'bad' puede ser un sitio peligroso, o simplemente malo en algún sentido, que puede no estar relacionado con la seguridad.
But at the same time, hundreds of new sites (dangerous or not) are created. Netcraft reports from November \cite{netcraft:site} show that there are about 950 million of active websites. BIn addition McAfee reported \cite{mcafee:site} that, at the end of the first quarter of 2014, there were more than 18 million new suspect URLs (2 million associated domains), 
and also more than 250 thousand new pishing URL (almost 150 thousand associated domains).

With this scenario, companies need to be able to update their blacklists and whitelists not only considering what is reported by security servers but also by learning from the connections that the employees made in the past and were known as dangerous. 
% Antonio - esto no es que sea necesario, es que es la idea que se plantea. Hasta el momento no se ha hecho así. Yo lo diría de esa forma, planteándolo como una idea novedosa.
This way, anomalous situations can be detected and also classified as candidates for the blacklist or the whitelist.

% ¿y si se introduce este párrafo como sigue?  [Pedro]
% In this work, using categorical classifiers to classify web sites is proposed.
This work shows that this can be achieved by means of categorical classifiers. 
% Antonio - Yo no diría que el trabajo muestra algo. Mejor decir que se presenta un enfoque basado en eso y que los resultados son prometedores, es decir, muestran indicios de que funcionaría, pero es muy complicado hacer un clasificador en el que se pueda basar una empresa para añadir nuevas URLs tal cual... esto sería una herramienta más, que ayudaría, pero que no sustituiría al proceso que se haga actualmente, no sé si me explico.
It is possible to gather information from the log files that the companies proxies store, and also from the set of Corporate Security Policies (CSPs), in order to be able to label the entries of the log and obtain a data set.
% Esto no tiene nada que ver. Las políticas de seguridad no son necesarias. Nos basamos en listas blancas y negras, que definen, por defecto, reglas de seguridad muy simples, que son las que usamos en su momento. En el primer caso esto nos lo vendieron como CSPs, pero no son más que la correspondencia de dichas listas. Yo no mencionaría esto.
% Esto es parte de la metodología más bien.

 That data set has been the one used for training and testing a set of chosen classifiers, being all of them capable of support categorical data given the nature of the information in a URL connection log file. We will show how the classifiers have obtained rules, for classifying unknown URLs, that are built with more antecedents than the URL only, as normal Black and White lists do. % yo los pondría en minúsculas [Pedro]

% Antonio - hay que completar esta sección. Normalmente no hace falta describir el proceso que se seguirá (con mucho detalle), ni lo que se mirará en los resultados, sino lo que se ha obtenido de forma general.
% En todo caso, habría que hacer hincapié en la selección de características, en que nos apoyaremos en el usuario para hacer dicha selección, que se considerarán sesiones, etc.
% También mencionar que el trabajo extiende el del ECTA y cómo lo extiende.

The paper is structured as follows: related work about enterprise network security, how Data Mining and Machine Learning techniques help to maintain it, and about URL classification methods, is discussed in Section 2. Section 3 provides a deeper description of the problem we aim to solve, as well as the used data and their composition. Then, after presenting the different operations performed over the initial data in Section 4, Section 5 describes the different sets of conducted experiments, in which the resulted datasets from Section 4 are tested with different classifiers. Results of those experiments are addressed in Section 6. Finally, in Section 7 we look over the conclusions of this work and give an overview about how to further continue our research in the future.

%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%   BACKGROUND AND RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Background and related work}
\label{sec:background_sota}


%Network and internet security in enterprises


%Data

The datasets that have been traditionally used for this kind of studies are the original KDD 99 Cup Dataset \cite{kddcup99}, and its improved version NSL KDD \cite{nslkdd}. Both are detailed by Tavallaee, Mahbod et al. \cite{tavallaee2009detailed}, and the first consists of a group of connection logs from a normal network, with simulated attack traffic added. The second version, which may be called as improved or refined, had the same log of connections from the initial dataset, but having removed the redundant traffic. This means that there might have been duplicated records in the original dataset that could mislead the results after the training and testing processes.

The main advantage in the current work is that the data correspond to a log directly extracted from an actual company, thus these are more accurate datasets than those which include simulated traffic. These data have been also pre-processed for removing incorrect and duplicated data, as will be deeply explained in Section \ref{sec:methodology}. 

%URL classification

The study dedicates an important part to URL, considering its different parts, the potential additional information that it provides, and the way it influences in a classification process. In this line, there exist some works related with the study of URLs. The most interesting ones are focused in trying to identify malicious sites (like the ones that want to perform a pishing attack). 
% Antonio - poner aquí los trabajos que traten de identificación de URLs o sitios maliciosos.

There are also other researches studying the URL lexical features (as it is proposed in this work), since we consider this kind of approaches better than to download and process the web (the thing that in fact is trying to be avoided). 
% Antonio - lo que está entre paréntesis no sé si está bien expresado.

Hence, Kan and Thi \cite{Kan_URL05} focused their work in lexical features 
% Antonio - ¿en qué exáctamente de las 'lexical features'?¿extracción, generación?
in order to classify as dangerous, URLs that were not previously in blacklists servers. They gathered features such as the URI components, length, orthographic data, or segments by entropy reduction. Their results were close to 95\% of accuracy.
% Antonio - ¿sobre qué datos trabajaron? Este artículo parece similar al nuestro y hay que buscarle los puntos débiles o lo que se mejora en el que presentamos nosotros.

On the other hand, our work not only focuses in lexical features of the requested URL, but also in other data that appears in the log files. 
% Antonio - Esto se refiere al nuestro, ¿no? Entonces hay que recalcar varias cosas: 1) que las 'lexical features' que consideramos son muchas más o, al menos, son diferentes (y puede que mejores), 2) que se considera un factor de influencia del usuario, 3) que el uso de otras variables del log mejora las referencias en las que se basa la clasificación.

Zhang et al. \cite{Zhang_cantina07}, with the software named CANTINA, detected pishing URLs by studying lexical features, along with content related features, and a WHOIS query (obtaining the date when the domain were registered, which if it is too new, it can be less trustful). They also got a 95\% of accuracy.
% Antonio - No todo se basa en porcentajes de acierto, y menos en este trabajo. Yo destacaría más bien si los resultados eran interesantes (para la empresa), si eran fiables, etc.

Maybe the most important work in this research line was presented by J. Ma et al. \cite{Ma_Url11}, whose aim was to detect malicious URLs, mainly related with pishing attacks through e-mailing, but without processing the content or other private data of the user. They extracted information from the lexical features (62\% of the total amount of gathered features), and also from the host that has the URL. They performed the study over 100 days, and that they worked with a quantity of almost 2 million of features. In addition, they implemented an online classifier (instead of a batch one), and obtained a 99\% of accuracy.
% Antonio - hay que intentar destacarse de este trabajo, señalar que nuestros datos son reales, que nuestras características son mejores, que se hace una selección user-based, etc. Alabar un trabajo no es buena idea si lo que se destaca son cosas en las que sobresale respecto al tuyo.


%Crime data mining and forensics in enterprise (?)

% Antonio - No sé si esto tiene cabida en el trabajo. Quizá no, pero no estoy muy ducho en computer forensics para decidirlo. En todo caso hay que introducir el tema, no ponerlo de golpe.

On the one hand, DM 
% Antonio - DM supongo que es Data Mining. Esta es la primera vez que se nombra y habría que poner el término completo
helped to develop new solutions to computer forensics \cite{DeVel2001}, being the researchers able to extract information from large files with events gathered from infected computers. Another important advance took place after the 9/11 events, when \textit{clustering techniques} and \textit{social network analysis} started to be performed in order to detect pontential crime networks \cite{Hsinchun2003}.
On the other hand, and more focused on the user side like our approach, there exist some user-centric solutions to problems like user authentication in a personal device, who Greenstadt and Beal \cite{cognitive_security_08} proposed to address using collected user biometrics along with machine learning techniques.

%feature extraction

% Antonio - Esto es importante. Hay que mirar también métodos que se enfoquen en el usuario.

% Machine Learning

% Antonio - Si quieres meter cosas de estas en el estado del arte, en el artículo del GECCO hay un montón de referencias al uso de DM + ML en seguridad en la empresa. Incluso algunas pueden servir para mejorar algún párrafo anterior.

%-------------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  PROBLEM DEFINITION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------------------

\section{Problem definition}
\label{sec:problem_data}

%*** Definir el problema a resolver y describir los datos que se van a manejar en el mismo.
The problem this work addresses is the ability of self-adaptation that company network security systems have, for many new dangerous websites emerge every day. Corporate Security Policies (CSPs) may include Black and White lists
% Antonio - usar el término que se decida para las listas
 to cope with dangerous situations, but always for known and already classified websites. 
Thus, the goal of this paper is to demonstrate that is possible to automatically classify, as allowed or denied, a URL that was not in a blacklist or a whitelist. 
% Antonio - Esto no hay que demostrarlo, esto a se ha hecho, como se ha dicho en el SotA. Lo que se quiere es proponer una metodología de cómo hacerlo, basándose además en el usuario, para obtener resultados que sean precisos y sobre todo fiables.
Moreover the final application of an `ALLOW' or `DENY' label will depend on additional features of that URL, such as lexical, or contextual, for instance, going then a step beyond the Black and White lists.

The (anonymised) data used in the study have been provided by an actual Spanish company.
% Antonio - 'data' en este contexto se usa como plural
They have been extracted from a log file with the connections made by employees of the company. The data inside this log file were gathered along a period of two hours, from 8.30 to 10.30 am (30 minutes after the workday starts), monitoring the activity of all the employees in a medium-size Spanish company (80-100 people), obtaining 100000 patterns. We consider this dataset quite complete because it contains a very diverse amount of connection patterns, going from personal (traditionally addressed at the first hour of work) to professional issues (the rest of the day). This log contains the typical information of an HTTP request, it is gathered by the company Squid proxy \cite{squid:site}. 
% Antonio - Las cuestiones de formatos e implementación no son relevantes en este tipo de trabajos.

There is also a set of Corporate Security Rules, most of them corresponding to black and whitelists decisions, and thus, denying or allowing the access to a specific URL based on its \textit{core domain}. However, there are a few of them also considering other features in the conditions, such as...
% Antonio - poner oros factores que se tenían en cuenta, como que si era un vídeo no ocupase más de X bytes, etc (si lo estimamos oportuno)
These rules have been provided by the same company.

In order to have enough information to train the classifiers, we have taken as initial features the same fields that the Squid log provides, being those of an HTTP request: the HTTP reply code and method, the time when the connection was made and how much did it take to the server to answer, the IP addresses of the client and requested server, the content type of the webpage and its size in bytes, and the complete URL string.
The URL has the following structure:

\begin{verbatim}
http://www.subdomain5.....subdomain1.url_core.url_TLD/folder1/.../
/filename.filename_extension
\end{verbatim}

Thus a lot of additional features can be obtained from the URL string, which can be lexical or non-lexical. We name as lexical those features which are extracted from the characteristics of the URL string \cite{Kan_URL05}, such as URL string length, how many subdomains it has, how many letters or numbers are in the whole string, or if it contains folders or parameters, for instance. The non-lexical features of a URL can vary from the type of content of the website the user is accessing, to the properties of the host which contains the site \cite{Ma_Url11}. Here we have focused in lexical features, as most of the reviewed related works use this type of variables in their classification studies, and also for computing time reasons. 
When a decision for a new URL must be made, it is much faster to analyse its lexical features than to gather its non-lexical properties.

% Antonio - antes de esto hay que justificar por qué nos interesa el usuario (en la intro también). Es decir, en la seguridad actual los usuarios son muy importantes, son una fuente de peligro 'desde dentro' que hay que controlar.
% Los sistemas de seguridad se han planteado normalmente para evitar ataques externos, pero hay que cambiar el enfoque e ir considerando al empleado en el bucle
It is important to point out that, from all the information which can be gathered from each entry of the log, not every field (or feature) is dependant on the user. Table \ref{featuretype} shows the set of initial features extracted from the log of connections. For each feature, its type is indicated and also the dependency with the user. 
This first study over the features can help to the feature selection, because what matters for identifying non desirable users behaviour, would be the information that depends on the user.
% Antonio - Esta segunda frase debe ir en otro lado, parte arriba (en esta sección y en la intro), donde se hable del usuario como amenaza, para justificar este trabajo y diferenciarlo del resto. Y otra parte (la de feature selection) más adelante, en la metodología.

\begin{table*}[htpb]
\centering
 \caption{\label{featuretype} Extracted features from each entry of the Log and its dependance or independence on the user behaviour.}
{\scriptsize
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature name} & \textbf{Feature type} & \textbf{Relationship with user behaviour}\\ 
\hline
\texttt{http\_reply\_code} & Categorical & Independent \\ 
\texttt{http\_method} & Categorical & Independent \\ 
\texttt{duration\_milliseconds} & Numeric & Independent \\
\texttt{content\_type\_MCT} & Categorical & Independent \\ 
\texttt{content\_type} & Categorical & Independent \\ 
\texttt{server\_or\_cache\_address} & Categorical & Independent \\
\texttt{time} & Date & Dependent \\ 
\texttt{squid\_hierarchy} & Categorical & Independent \\ 
\texttt{bytes} & Numeric & Independent \\  
\texttt{URL\_length} & Numeric & Dependent \\  
\texttt{letters\_in\_URL} & Numeric & Dependent \\  
\texttt{digits\_in\_URL} & Numeric & Dependent \\  
\texttt{nonalphanumeric\_chars\_in\_URL} & Numeric & Dependent \\  
\texttt{url\_is\_IP} & Boolean & Dependent \\  
\texttt{url\_has\_subdomains} & Boolean & Dependent \\  
\texttt{num\_subdomains} & Numeric & Dependent \\  
\texttt{subdomain5} & Categorical & Dependent \\  
\texttt{subdomain4} & Categorical & Dependent \\  
\texttt{subdomain3} & Categorical & Dependent \\  
\texttt{subdomain2} & Categorical & Dependent \\  
\texttt{subdomain1} & Categorical & Dependent \\  
\texttt{url\_core} & Categorical & Dependent \\  
\texttt{url\_TLD} & Categorical & Dependent \\  
\texttt{url\_has\_path} & Boolean & Dependent \\  
\texttt{folder1} & Categorical & Dependent \\  
\texttt{folder2} & Categorical & Dependent \\  
\texttt{path\_has\_parameters} & Boolean & Dependent \\  
\texttt{num\_parameters} & Numeric & Dependent \\  
\texttt{url\_has\_file\_extension} & Boolean & Dependent \\  
\texttt{filename\_length} & Numeric & Dependent \\  
\texttt{letters\_in\_filename} & Numeric & Dependent \\  
\texttt{digits\_in\_filename} & Numeric & Dependent \\  
\texttt{other\_char\_in\_filename} & Numeric & Dependent \\  
\texttt{file\_extension} & Categorical & Dependent \\  
\texttt{url\_protocol} & Categorical & Dependent \\ 
\texttt{client\_address} & Categorical & Dependent \\
\hline
\end{tabular}
}
\end{table*}

% Antonio - decir que el problema se transforma en un problema de clasificación y justificarlo. Ya se ha dicho antes, pero es en esta sección en la que hay que exponerlo y defenderlo.

Then, every original security rule covers a number of entries, and so a class is applied to them. This class is a label with two possible values: `allow', or `deny'.
% Antonio - unas veces están en mayúscula y otras no
An `allow' class is applied to an entry when it suffices the conditions of a rule which permits the connection, or because the URL is included in the whitelist. On the contrary, the `deny' class is assigned to those entries that are not permitted by the company, then, they will fit the rules that forbid those connections, or their URLs will be included in the blacklist.

Even though the whole methodology is detailed in the next Section, it is important to note now that in addition to the creation of a dataset with the labelled entries, it must be studied how to divide it in training and test datasets. For this reason, before we reach our main goal, we need to find a proper way to treat the initial dataset, and also to obtain a test setup that maximises the accuracy results and provides a good set of rules to demonstrate this work main purpose.
% Antonio - Lo del balanceo ya no se va a hacer en este trabajo, creo yo.

%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Methodology}
\label{sec:methodology}

This Section is devoted to look over the steps that have been followed the realisation of different sets of experiments, in order to perform a deep study of the classifiers and data configurations. An experiment consists of obtaining a dataset configuration, a certain classifier, and trainining and test it to see the accuracy results.

The first and usual step \cite{Frank2011} is to preprocess all data and extract the information to have it in a way easy to work with. From the initial files, a connection log and a set of rules, we obtain a dataset with the data being processed and the rule decisions being applied. The nature of this dataset forces the application of some techniques for producing other datasets. In addition, the very results of the experiments justify the creation of new datasets, as will be explained in further subsections.

%*** Describir la metodología a seguir para la creación de los juegos de datos:
%- preprocesado
%- etiquetado para componer datos a clasificar
%- técnicas de balanceo

%*** Describir los 3 grandes bloques y justificarlos:
%- Datos iniciales
%- Características extraídas
%- Sesiones

All Java code for the realisation of this work is available at Github \cite{github:site}. % Short url http://git.io/4cQYFQ

%---------------------------------------------------------------------
\subsection{Initial Data}
\label{subsec:initial_data}

In order to have the extracted information in a proper way to train and test the classifiers, a preprocessing is performed over both log and rules files. From now on, we treat Black and White lists as set of rules too, given the fact that we can express every entry of those lists as:

\begin{verbatim}
IF
url = some_allowed_or_denied_url
THEN
allow/deny
\end{verbatim}

In a certain rule, the antecedents are expressed in the first place. They are the conditions that must be met in order to apply a decision. The antecedent for black and white lists and in the example is \textit{url = some\_allowed\_or\_denied\_url}, but there can be as much conditions as needed in a rule. Specialization of a rule is when it has a lot of conditions and covers a few patterns, while generalisation of rules means to have rules with less conditions but covering more patterns. %no estoy muy segura de esto último REVISAR
% Yo diría que es correcto, pero no he leído el resto, no sé si viene
% a cuento - JJ

Our main goal is to be able to obtain rules with the ability to label connection patterns as allowed or denied, and that the conditions of those rules are not dependant on the URL. Thus, after the preprocessing and having extracted all the knowledge from the log and the rules, the entries in the Log can be labelled.

The initial log file has 100000 connection patterns. At the end of the labelling process, a dataset is formed with 58778 connection patterns, now with an `allow' or `deny' class assigned. The labelling process yields to 39044 allowed patterns and 19734 denied patterns. This dataset will be used for training and testing the classifiers, but before, is the one used for a first overview of the classifiers we intend to work with.

We have focused in two types of classifiers: rule based and tree based. This decision was based in the fact that 50\% of the features in every instance of the dataset is categorical. In order to measure the distance between two values of a categorical feature, one can use the Hamming or the Levenshtein distance \cite{chudnovsky2008method}, but this could work only for some features such as subdomains, for example. Furthermore, there are also numerical features. Then, we needed classifiers able to work with both numerical and categorical feature types.
Besides, they were chosen because its a set of rules what we aim to obtain. Also, the set of rules that these type of classifier generate from the training dataset helps with the feature selection.
This work has been developed in Java along with the Weka tool \cite{weka:site}, as it has several different classifiers of each type.

As can be seen in the ratio between allowed and denied patterns, the dataset is unbalanced and that can bias classification results \cite{maimon2005data}. There exist numerous techniques for balancing datasets, but here we have used two techniques that are mentioned in \cite{imbalance_techniques_02}. More specifically, these techniques are:

\begin{itemize}
   \item \textbf{Undersampling}. By this technique, the patterns of the majority class are reduced proportionally until both classes have the similar amount of patterns. This pattern removal has been performed randomly \cite{random_undersampling_08} and for that reason, three different balanced datasets have been created and tested, so that at the end the results have a mean and a standard deviation. Of course, removing patterns has a main disadvantage, which is the loss of information.
   \item \textbf{Oversampling}. On the contrary, with this method the patterns in the minority class are increased. Sometimes the difference between classes is filled with synthetic data \cite{smote_02}. When the features are numerical, is easier to introduce synthetic data similar to the extisting, because the differences between patterns can be numerically measured. In our case, most of the features are categorical so that is hard to create synthetic data. Then, the connection patterns of the minority class (allow) are doubled.
\end{itemize}

Then, having three datasets with the initial data, one unbalanced and two balanced with different methods, and 9 rule based plus 7 tree based classifiers, the datasets are tested by a 10-fold cross-validation process. This means that every dataset is divided in 10 equal parts and that the classifier will be trained an tested 10 times: each time it is trained with 9 of the 10 parts and the 10th is for testing \cite{Frank2011}. After the 10 generations, the results are calculated by averaging. This process is necessary to see classifiers' behaviour with the type of data we are working with. Also, Naïve Bayes classifier \cite{Bayesian_Classifier_97} has been included as a reference for both its simplicity and good results with all data types. Table \ref{tabresults_todos} shows %something that I'll explain on monday

\begin{table}[htpb]
\centering 
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & Undersampling & Oversampling \\ 
\hline
Naïve Bayes & 91.02 $\pm$ 0.10 & 91.77 \\ 
\cline{1-1}
Conjunctive Rule & 60.00 $\pm$ 0.14 & 60.02 \\ 
\cline{1-1}
Decision Table & 94.31 $\pm$ 0.20 & 90.29 \\ 
\cline{1-1}
DTNB & 94.92 $\pm$ 0.14 & 95.65 \\ 
\cline{1-1}
JRip & 90.03 $\pm$ 0.07 & 92.47 \\ 
\cline{1-1}
NNge & \textbf{96.47} $\pm$ 0.02 & \textbf{98.76} \\ 
\cline{1-1}
One R & 93.53 $\pm$ 0.08 & 93.70 \\ 
\cline{1-1}
PART & \textbf{96.35} $\pm$ 0.09 & \textbf{97.54} \\ 
\cline{1-1}
Ridor & 86.60 $\pm$ 0.55 & 89.87 \\ 
\cline{1-1}
Zero R & 51.22 $\pm$ 0.16 & 51.26 \\ 
\cline{1-1}
AD Tree & 77.65 $\pm$ 0.08 & 77.68 \\ 
\cline{1-1}
Decision Stump & 60.00 $\pm$ 0.14 & 60.02 \\ 
\cline{1-1}
J48 & \textbf{96.99} $\pm$ 0.04 & \textbf{98.00} \\ 
\cline{1-1}
LAD Tree & 78.93 $\pm$ 1.71 & 79.97 \\ 
\cline{1-1}
Random Forest & \textbf{96.94} $\pm$ 0.07 & \textbf{98.84} \\ 
\cline{1-1}
Random Tree & 95.59 $\pm$ 0.40 & 98.35 \\ 
\cline{1-1}
REP Tree & \textbf{96.74} $\pm$ 0.04 & \textbf{97.67} \\ 
\hline
\end{tabular}
}
\caption[Global classification methods ranking. Classifiers are trained and tested by crossvalidation.]{\label{tabresults_todos} Results of all tested classification methods on balanced data. The best ones are marked in boldface.}
\end{table}


%*** Describir los primeros datos y procesamientos (eliminar duplicados, agrupar redundantes, etc)


%---------------------------------------------------------------------
\subsection{Extracting Features}
\label{subsec:extracting_features}

*** Describir y justificar las características extraídas de los datos iniciales.

%---------------------------------------------------------------------
%\subsection{Grouping in Sessions}
%\label{subsec:sessions}

%*** Describir y justificar el proceso de agrupamiento en sesiones

% Esto al final no entra en este artículo



%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS AND RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Experiments and Results}
\label{sec:experiments}

*** Describir los experimentos realizados, en cada bloque.
*** Analizar los resultados obtenidos en cada bloque

%---------------------------------------------------------------------
\subsection{Initial Data Results}
\label{subsec:initial_data_results}

*** Experimentos y resultados sobre el conjunto original y los conjuntos 'refinados' (TLD, sin duplicados, etc)


%---------------------------------------------------------------------
\subsection{Extracted Features Results}
\label{subsec:extracted_features_results}

*** Experimentos y resultados sobre el conjunto con las nuevas características


%---------------------------------------------------------------------
\subsection{Sessions Results}
\label{subsec:sessions_results}

*** Experimentos y resultados sobre el conjunto agrupando por sesiones



%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   DISCUSSION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

*** Comentar los resultados de cada método y analizar las ventajas e inconvenientes de cada uno, así como las mejoras que se hayan conseguido con la extracción de características y el agrupamiento por sesiones


%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Conclusions and Future Work}
\label{sec:conclusions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ACKNOWLEDGEMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgements}
This work has been supported by the European project MUSES (FP7-318508).

\bibliographystyle{elsarticle-num}
\bibliography{review_muses,data_mining_urls,ci_security_rules}

\end{document}
